{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model 3: GRU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DAbbott93/Dean-Abbott--Dissertation/blob/main/Model_3_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRFJlFVbCB9W"
      },
      "source": [
        "# GRU model using BERT embeddings for Sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyGcZomRCYPK"
      },
      "source": [
        "In this notebook I use the pretrained BERT transormer model (from the transformers library) as embedding layers for our GRU model.  I will freeze BERT and only train the remainder of the model which learns from the representations produced by the transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-L7PdPZ9DRpk"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5ShmlJawKW7",
        "outputId": "33c84808-1d8c-4a2c-c71f-3ab890ea0fab"
      },
      "source": [
        "!pip install torch==1.6.0 torchvision==0.7.0 torchtext==0.7.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.6.0\n",
            "  Downloading torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8 MB)\n",
            "\u001b[K     |█████████████████████████████   | 679.1 MB 45.2 MB/s eta 0:00:02\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "\u001b[?25hTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 180, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py\", line 199, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 319, in run\n",
            "    reqs, check_supported_wheels=not options.target_dir\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 128, in resolve\n",
            "    requirements, max_rounds=try_to_avoid_resolution_too_deep\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 473, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 341, in resolve\n",
            "    name, crit = self._merge_into_criterion(r, parent=None)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 172, in _merge_into_criterion\n",
            "    if not criterion.candidates:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/structs.py\", line 139, in __bool__\n",
            "    return bool(self._sequence)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in __bool__\n",
            "    return any(self)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 129, in <genexpr>\n",
            "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 33, in _iter_built\n",
            "    candidate = func()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 205, in _make_candidate_from_link\n",
            "    version=version,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 312, in __init__\n",
            "    version=version,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 151, in __init__\n",
            "    self.dist = self._prepare()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 234, in _prepare\n",
            "    dist = self._prepare_distribution()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 318, in _prepare_distribution\n",
            "    self._ireq, parallel_builds=True\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/prepare.py\", line 508, in prepare_linked_requirement\n",
            "    return self._prepare_linked_requirement(req, parallel_builds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/prepare.py\", line 552, in _prepare_linked_requirement\n",
            "    self.download_dir, hashes\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/prepare.py\", line 243, in unpack_url\n",
            "    hashes=hashes,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/prepare.py\", line 102, in get_http_url\n",
            "    from_path, content_type = download(link, temp_dir.path)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/network/download.py\", line 157, in __call__\n",
            "    for chunk in chunks:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/progress_bars.py\", line 152, in iter\n",
            "    for x in it:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/network/utils.py\", line 86, in response_chunks\n",
            "    decode_content=False,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/urllib3/response.py\", line 576, in stream\n",
            "    data = self.read(amt=amt, decode_content=decode_content)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/urllib3/response.py\", line 502, in read\n",
            "    self._init_decoder()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/urllib3/response.py\", line 377, in _init_decoder\n",
            "    if content_encoding in self.CONTENT_DECODERS:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/progress_bars.py\", line 107, in handle_sigint\n",
            "    self.original_handler(signum, frame)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main.py\", line 71, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 104, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 213, in _main\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1366, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1514, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1524, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1586, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 894, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.7/logging/handlers.py\", line 71, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1127, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1025, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 869, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/logging.py\", line 130, in format\n",
            "    formatted = super().format(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 616, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 561, in formatException\n",
            "    sio = io.StringIO()\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjfOKZboDdTd"
      },
      "source": [
        "Set the seed to achieve reproducibilty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPF-gN_rF2fs"
      },
      "source": [
        "import torch\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QGd96rW1FAM",
        "outputId": "35c1354f-9bd9-4960-e861-c86828d429a5"
      },
      "source": [
        "# check whether cuda is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM1AKS9vGlUU"
      },
      "source": [
        "Tokenise the data into the required format for BERT. Use the BertTokenizer from the transformers library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e1gogvKHPZ0",
        "outputId": "29c6a70b-880d-4477-b66e-f7e92efc031d"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 40.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 49.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 38.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9xf2vO1HDWO"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "# Use bert base uncased tokeniser\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGzHDivqHrIF"
      },
      "source": [
        "Mark the mark the beginning of each with ([CLS]) and the end of each sentence with ([SEP]. Also, add padding and unkown tokens. This is the required format for BERT inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Jpb5_6ZK_oU"
      },
      "source": [
        "init_token = tokenizer.cls_token\n",
        "eos_token = tokenizer.sep_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "\n",
        "print(init_token, eos_token, pad_token, unk_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxT7nuYzM504"
      },
      "source": [
        "Get indexes for the special tokens from the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIDJZkEhM6XR"
      },
      "source": [
        "init_token_idx = tokenizer.cls_token_id\n",
        "eos_token_idx = tokenizer.sep_token_id\n",
        "pad_token_idx = tokenizer.pad_token_id\n",
        "unk_token_idx = tokenizer.unk_token_id\n",
        "\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HviXOEduLeQF"
      },
      "source": [
        "BERT was trained on a defined maximum length, therefore set our max length to this value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oduVVF-kLvOm"
      },
      "source": [
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "\n",
        "print(max_input_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWVDzp8hMXJv"
      },
      "source": [
        "Define a method for tokenization.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt3g9hRzMXWZ"
      },
      "source": [
        "def tokenize_and_cut(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence) \n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2-JsjwNNoeS"
      },
      "source": [
        "Define the fields for how the data should be processed.  Use the TEXT field to define how the review should be processed, and the LABEL field to process the sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJRc_qCp0ptC"
      },
      "source": [
        "!pip install torchtext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qceBesymNyqi"
      },
      "source": [
        "from torchtext import data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN0VyLbuv4WL"
      },
      "source": [
        "TEXT = data.Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = tokenize_and_cut,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  init_token = init_token_idx,\n",
        "                  eos_token = eos_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQnxUWdFVV07"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIf-9B0BVeIr"
      },
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "dataset= load_dataset(\"hope_edi\", \"english\")\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mugw-xLkV-B6"
      },
      "source": [
        "Convert to dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91LEqO-Kf8Oo"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6h2Eh5TV-TL"
      },
      "source": [
        "from pandas import DataFrame\n",
        "\n",
        "#Training datset\n",
        "df_train=DataFrame({'text':dataset['train']['text'], 'label': dataset['train']['label']})\n",
        "print(df_train.shape)\n",
        "\n",
        "df_train['label'] = df_train['label'].replace([1], \"negative\")\n",
        "df_train['label'] = df_train['label'].replace([0], \"positive\")\n",
        "df_train.to_csv('/content/drive/MyDrive/Hope_Dataset/train.tsv', sep=\"\\t\",index=False)\n",
        "\n",
        "\n",
        "#Validation dataset\n",
        "df_val=DataFrame({'text':dataset['validation']['text'], 'label': dataset['validation']['label']}) \n",
        "x = df_val['label'].value_counts()\n",
        "df_val['label'] = df_train['label'].replace([1], \"negative\")\n",
        "df_val['label'] = df_train['label'].replace([0], \"positive\")\n",
        "df_val.to_csv('/content/drive/MyDrive/Hope_Dataset/test.tsv', sep=\"\\t\", index=False)\n",
        "\n",
        "df_val.head()\n",
        "df_train.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leLtQx7NyC9b"
      },
      "source": [
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axk20xS6ZGpA"
      },
      "source": [
        "df_val['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38aydT0vDc9y"
      },
      "source": [
        "df_train.sample(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UgchhiHXzU3"
      },
      "source": [
        "fields = [('text', TEXT), ('label', LABEL)]\n",
        "#loading custom dataset\n",
        "training_data=data.TabularDataset(path =\"/content/drive/MyDrive/Hope_Dataset/train.tsv\",format = 'tsv',fields = fields,skip_header = True)\n",
        "\n",
        "\n",
        "train_data, valid_data = training_data.split(split_ratio=0.8, random_state = random.seed(SEED))   #, random_state = random.seed(SEED)\n",
        "\n",
        "\n",
        "print(len(train_data))\n",
        "print(len(valid_data))\n",
        "\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "# No. of unique tokens in label\n",
        "print(\"Size of LABEL vocabulary:\", len(LABEL.vocab))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0Vx7WMPYTGw"
      },
      "source": [
        "Create Iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j6pSj7-YVbQ"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    sort_key=lambda x: len(x.text),\n",
        "    sort_within_batch=True,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4--Y0ddYb0A"
      },
      "source": [
        "# Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNQW-D_rYdTc"
      },
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "bert = BertModel.from_pretrained('bert-base-uncased') # it is important to use the same model and tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2VYnZ2TY3ZT"
      },
      "source": [
        "Define the model.\n",
        "The pretrained BERT model for the embedding layer.  The embdeeings are fed into the the LSTM model to produce a prediction for the sentiment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5APQucRcY3hx"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BERTGRUSentiment(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 bidirectional,\n",
        "                 dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = bert\n",
        "        \n",
        "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        \n",
        "        self.rnn = nn.GRU(embedding_dim,\n",
        "                          batch_first = True,\n",
        "                          hidden_dim,\n",
        "                          num_layers = n_layers,\n",
        "                          bidirectional = bidirectional,                \n",
        "                          dropout = 0 if n_layers < 2 else dropout)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):    \n",
        "                \n",
        "        with torch.no_grad():\n",
        "            embedded = self.bert(text)[0]            \n",
        "        \n",
        "        _, hidden = self.rnn(embedded)        \n",
        "        \n",
        "        if self.rnn.bidirectional:\n",
        "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1,:,:])                \n",
        "        \n",
        "        output = self.out(hidden)      \n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REbsBO84aJ8k"
      },
      "source": [
        "Define hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVpWR0WgaIyJ"
      },
      "source": [
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.25\n",
        "N_LAYERS = 2\n",
        "\n",
        "model = BERTGRUSentiment(bert,\n",
        "                         N_LAYERS,\n",
        "                         HIDDEN_DIM,\n",
        "                         OUTPUT_DIM,\n",
        "                         BIDIRECTIONAL,\n",
        "                         DROPOUT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL87rNbVabPE"
      },
      "source": [
        "Define method to find out how many  parameters the model has. Most of the parameters are for BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwYq_e5Barhd"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmcTft58a1zm"
      },
      "source": [
        "Set requires_grad attribute to False to freeze parameters for BERT\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8EYl1RKa0iF"
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEVMiCTqbXzX"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3luMILebg_m"
      },
      "source": [
        "Print the names of the trainable parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k0nWd9Bbh-v"
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzdg2RlxbnMP"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vha8AYjJbtof"
      },
      "source": [
        "Define optimizer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1nnTM4Xbmen"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI23ygUlcAOv"
      },
      "source": [
        "Place the model and criterion onto the GPU "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ_Jp3-8sPpz"
      },
      "source": [
        "# check whether cuda is available\n",
        "if torch.cuda.is_available():    \n",
        "    # If a GPU is available tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    # Print that a GPU is available and its name\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If a GPU is not available print the following statement\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jYhW2h6cAWV"
      },
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e__j0N7Xdicx"
      },
      "source": [
        "Define method for calculating accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzDKroWwdi3i"
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsvkobMLdtYe"
      },
      "source": [
        "Define model for performing a training epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE_23PD2dt3z"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr6R8HMJeAmK"
      },
      "source": [
        "Define a method to performing an evaluation epoch and calculating how long a training/evaluation epoch takes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ppgq0EA5GjtE"
      },
      "source": [
        "def f1_loss(y_pred:torch.Tensor, y_true:torch.Tensor, is_training=False):\n",
        "    '''Calculate F1 score. Can work with gpu tensors'''\n",
        "   \n",
        "    assert y_true.ndim == 1\n",
        "    assert y_pred.ndim == 1 or y_pred.ndim == 2\n",
        "    \n",
        "    if y_pred.ndim == 2:\n",
        "        y_pred = y_pred.argmax(dim=1)\n",
        "    \n",
        "    y_pred = torch.round(torch.sigmoid(y_pred))\n",
        "   \n",
        "   \n",
        "\n",
        "    \n",
        "    tp = (y_true * y_pred).sum().to(torch.float32)\n",
        "    tn = ((1 - y_true) * (1 - y_pred)).sum().to(torch.float32)\n",
        "    fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n",
        "    fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n",
        "    \n",
        "    epsilon = 1e-7\n",
        "    \n",
        "    precision = tp / (tp + fp + epsilon)\n",
        "    recall = tp / (tp + fn + epsilon)\n",
        "    \n",
        "    f1 = 2* (precision*recall) / (precision + recall + epsilon)\n",
        "    f1.requires_grad = is_training\n",
        "    return f1, precision, recall, tp, tn, fp, fn\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z7jSbA0eBEK"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    tp = 0\n",
        "    tn = 0 \n",
        "    fp = 0\n",
        "    fn =0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            #print('label:', batch.label)\n",
        "            #print('preds;', predictions)\n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "            f1, precision, recall, tp, tn, fp, fn= f1_loss(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            tp +=tp.item()\n",
        "            fp +=fp.item()\n",
        "            tn +=tn.item()\n",
        "            fn +=fn.item()\n",
        "\n",
        "\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), tp, tn, fp, fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb2RuuPLeRps"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7h1vL_QeXzd"
      },
      "source": [
        "Train the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGbcnyyGecdt"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc, tp, tn, fp, fn = evaluate(model, valid_iterator, criterion)\n",
        "        \n",
        "    end_time = time.time()\n",
        "        \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFFwCqA6fUjM"
      },
      "source": [
        "Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCBKch8PfVHQ"
      },
      "source": [
        "unseen_data = data.TabularDataset(path=\"/content/drive/MyDrive/Hope_Dataset/test.tsv\",format='tsv', fields= [('text', TEXT), ('label', LABEL)], skip_header=True)\n",
        "\n",
        "\n",
        "  # loading custom dataset\n",
        "unseen_train_data, unseen_data = unseen_data.split(split_ratio=0.99,\n",
        "                                                      random_state=random.seed(\n",
        "                                                          SEED))  \n",
        "\n",
        "print(len(unseen_train_data))\n",
        "print(len(unseen_data))\n",
        " \n",
        "LABEL.build_vocab(unseen_train_data)\n",
        "\n",
        "  \n",
        "\n",
        "unseen_train_data_iter, unseen_data_iter = data.BucketIterator.splits((unseen_train_data, unseen_data),\n",
        "                                                                        batch_size=256,\n",
        "                                                                        sort_key=lambda x: len(x.text),\n",
        "                                                                        sort_within_batch=True, device=device)\n",
        "                                                                  \n",
        "\n",
        "model.load_state_dict(torch.load('trip_140521.pt'))\n",
        "\n",
        "\n",
        "def evaluate_testset(model, unseen_train_data_iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    tp_total = 0\n",
        "    tn_total = 0 \n",
        "    fp_total = 0\n",
        "    fn_total =0\n",
        "    total_no_inputs = 0\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in unseen_train_data_iter:\n",
        "            total_no_inputs += 256        \n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "            f1, precision, recall, tp, tn, fp, fn= f1_loss(predictions, batch.label)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            tp_total +=tp.item()\n",
        "            fp_total +=fp.item()\n",
        "            tn_total +=tn.item()\n",
        "            fn_total +=fn.item()\n",
        "           \n",
        "\n",
        "    return epoch_loss / len(unseen_train_data_iter), epoch_acc / len(unseen_train_data_iter), tp_total, tn_total, fp_total, fn_total \n",
        "\n",
        "\n",
        "unseen_test_loss, unseen_test_acc, tp_total, tn_total, fp_total, fn_total = evaluate_testset(model, unseen_train_data_iter, criterion)\n",
        "print(f'Unseen Test Loss: {unseen_test_loss:.3f} | Unseen Test Acc: {unseen_test_acc * 100:.2f}%')\n",
        "print(  'tp:', tp_total, 'tn:', tn_total, 'fp:', fp_total, 'fn:', fn_total)\n",
        "prec = tp_total/(tp_total + fp_total)\n",
        "recall = tp_total/(tp_total + fn_total)\n",
        "print('Recall', recall)\n",
        "print('Prec', prec )\n",
        "F1 = 2*prec*recall/ (prec+recall)\n",
        "print('F1:', F1)\n",
        "print(\"finished\")   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sq-y1lN_FQj"
      },
      "source": [
        "\n",
        "\n",
        "Test the sentiment of random sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMroTAMA_FQj"
      },
      "source": [
        "def predict_sentiment(model, tokenizer, sentence):\n",
        "    model.eval()\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    prediction = torch.sigmoid(model(tensor))\n",
        "    return prediction.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jumzIHsd_FQj"
      },
      "source": [
        "predict_sentiment(model, tokenizer, \"i hate indians\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6WNUFXz_FQk"
      },
      "source": [
        "predict_sentiment(model, tokenizer, \"omg how can someone be that amazing\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbZYY4fKHCMj"
      },
      "source": [
        "#Transfer learning\n",
        "##Airline dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlKqlaQfrJcr"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V-fbvnZHfIE"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdAJzD41rJcs"
      },
      "source": [
        "df2 = pd.read_csv(\"/content/drive/MyDrive/Tweets.csv\",encoding='latin1')\n",
        "df2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI4__I8Of2-P"
      },
      "source": [
        "\n",
        "\n",
        "#Training datset\n",
        "\n",
        "df2[\"airline_sentiment\"] = df2[\"airline_sentiment\"].replace(\"neutral\", \"negative\")\n",
        "df2=df2[[\"text\",\"airline_sentiment\"]]\n",
        "df2.columns = ['text', 'label']\n",
        "df2.sample(10)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94FmVRJOhHJ1"
      },
      "source": [
        "df2.to_csv('/content/drive/MyDrive/AirlineTweet.tsv', sep=\"\\t\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OMc9hdfl4Os"
      },
      "source": [
        "df2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO5pgul1a2hq"
      },
      "source": [
        "unseen_data2 = data.TabularDataset(path=\"/content/drive/MyDrive/AirlineTweet.tsv\",format='tsv', fields = [('text', TEXT), ('label', LABEL)], skip_header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as8uYrXxfRD7"
      },
      "source": [
        "unseen_data2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jjgrvFCGjnu"
      },
      "source": [
        "\n",
        "eq=4,\n",
        "                  #  vectors=\"glove.6B.100d\"unseen_train_data, unseen_data = unseen_data2.split(split_ratio=0.99,\n",
        "                                                      random_state=random.seed(\n",
        "                                                          SEED))  # , random_state = random.seed(SEED)\n",
        "\n",
        "\n",
        "print(len(unseen_train_data))\n",
        "print(len(unseen_data))\n",
        "\n",
        "LABEL.build_vocab(unseen_train_data)\n",
        "\n",
        " \n",
        "\n",
        "unseen_train_data_iter, unseen_data_iter = data.BucketIterator.splits((unseen_train_data, unseen_data),\n",
        "                                                                        batch_size=256,\n",
        "                                                                        sort_key=lambda x: len(x.text),\n",
        "                                                                        sort_within_batch=True, device=device)\n",
        "                                                                  \n",
        "\n",
        "model.load_state_dict(torch.load('tut6-model.pt'))\n",
        "\n",
        "\n",
        "def evaluate_testset(model, unseen_train_data_iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    tp_total = 0\n",
        "    tn_total = 0 \n",
        "    fp_total = 0\n",
        "    fn_total =0\n",
        "    total_no_inputs = 0\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in unseen_train_data_iter:\n",
        "           \n",
        "            total_no_inputs += 256\n",
        "           \n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "           \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "            f1, precision, recall, tp, tn, fp, fn= f1_loss(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            tp_total +=tp.item()\n",
        "            fp_total +=fp.item()\n",
        "            tn_total +=tn.item()\n",
        "            fn_total +=fn.item()\n",
        "           \n",
        "\n",
        "    return epoch_loss / len(unseen_train_data_iter), epoch_acc / len(unseen_train_data_iter), tp_total, tn_total, fp_total, fn_total \n",
        "\n",
        "\n",
        "unseen_test_loss, unseen_test_acc, tp_total, tn_total, fp_total, fn_total = evaluate_testset(model, unseen_train_data_iter, criterion)\n",
        "print(f'Unseen Test Loss: {unseen_test_loss:.3f} | Unseen Test Acc: {unseen_test_acc * 100:.2f}%')\n",
        "print(  'tp:', tp_total, 'tn:', tn_total, 'fp:', fp_total, 'fn:', fn_total)\n",
        "prec = tp_total/(tp_total + fp_total)\n",
        "recall = tp_total/(tp_total + fn_total)\n",
        "print('Recall', recall)\n",
        "print('Prec', prec )\n",
        "F1 = 2*prec*recall/ (prec+recall)\n",
        "print('F1:', F1)\n",
        "print(\"finished\")   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM6zrXZ6mpjm"
      },
      "source": [
        "##Covid dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kEuEy0pm3Ag"
      },
      "source": [
        "df3 = pd.read_csv(\"/content/drive/MyDrive/coviddataset.csv\",encoding='latin1')\n",
        "df3.dropna(subset = [\"Sentiment\"], inplace=True)\n",
        "df3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKRhjwjjm3Ah"
      },
      "source": [
        "\n",
        "\n",
        "df3['Sentiment'] = df3['Sentiment'].map({'Positive':'negative', 'Extremely Positive':\"positive\",'Negative':\"negative\",'Extremely Negative':\"negative\",'Neutral':\"negative\"})\n",
        "df3=df3[[\"OriginalTweet\",\"Sentiment\"]]\n",
        "df3.columns = ['text', 'label']\n",
        "df3.sample(10)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUpJPXBQm3Ah"
      },
      "source": [
        "df3.to_csv('/content/drive/MyDrive/covid_test.tsv', sep=\"\\t\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wafmVgl7kg7n"
      },
      "source": [
        "df3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps4fRM7dm3Ah"
      },
      "source": [
        "unseen_data3 = data.TabularDataset(path=\"/content/drive/MyDrive/covid_test.tsv\",format='tsv', fields=[('text', TEXT), ('label', LABEL)], skip_header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlI355AVm3Ah"
      },
      "source": [
        "unseen_data3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QilxwIstm3Ai"
      },
      "source": [
        "unseen_train_data, unseen_data = unseen_data3.split(split_ratio=0.99,\n",
        "                                                      random_state=random.seed(\n",
        "                                                          SEED))  \n",
        "\n",
        "\n",
        "print(len(unseen_train_data))\n",
        "print(len(unseen_data))\n",
        "\n",
        "LABEL.build_vocab(unseen_train_data)\n",
        "\n",
        " \n",
        "\n",
        "unseen_train_data_iter, unseen_data_iter = data.BucketIterator.splits((unseen_train_data, unseen_data),\n",
        "                                                                        batch_size=256,\n",
        "                                                                        sort_key=lambda x: len(x.text),\n",
        "                                                                        sort_within_batch=True, device=device)\n",
        "                                                                  \n",
        "model.load_state_dict(torch.load('tut6-model.pt'))\n",
        "\n",
        "\n",
        "def evaluate_testset(model, unseen_train_data_iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    tp_total = 0\n",
        "    tn_total = 0 \n",
        "    fp_total = 0\n",
        "    fn_total =0\n",
        "    total_no_inputs = 0\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in unseen_train_data_iter:\n",
        "            \n",
        "            total_no_inputs += 256\n",
        "            \n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "            f1, precision, recall, tp, tn, fp, fn= f1_loss(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            tp_total +=tp.item()\n",
        "            fp_total +=fp.item()\n",
        "            tn_total +=tn.item()\n",
        "            fn_total +=fn.item()\n",
        "           \n",
        "\n",
        "    return epoch_loss / len(unseen_train_data_iter), epoch_acc / len(unseen_train_data_iter), tp_total, tn_total, fp_total, fn_total \n",
        "\n",
        "\n",
        "unseen_test_loss, unseen_test_acc, tp_total, tn_total, fp_total, fn_total = evaluate_testset(model, unseen_train_data_iter, criterion)\n",
        "print(f'Unseen Test Loss: {unseen_test_loss:.3f} | Unseen Test Acc: {unseen_test_acc * 100:.2f}%')\n",
        "print(  'tp:', tp_total, 'tn:', tn_total, 'fp:', fp_total, 'fn:', fn_total)\n",
        "prec = tp_total/(tp_total + fp_total)\n",
        "recall = tp_total/(tp_total + fn_total)\n",
        "print('Recall', recall)\n",
        "print('Prec', prec )\n",
        "F1 = 2*prec*recall/ (prec+recall)\n",
        "print('F1:', F1)\n",
        "print(\"finished\")   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPvAV7pmqAZ3"
      },
      "source": [
        "df3.value_counts"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}